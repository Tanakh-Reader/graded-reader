{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MORPHEMES = 'g_nme_utf8 g_pfm_utf8 g_prs_utf8 g_uvf_utf8 g_vbe_utf8 g_vbs_utf8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     2.46s T g_nme_utf8           from bhsa/tf/2021\n",
      "   |     2.52s T language             from bhsa/tf/2021\n",
      "   |     2.50s T ls                   from bhsa/tf/2021\n",
      "   |     0.38s T nametype             from bhsa/tf/2021\n",
      "   |     2.72s T st                   from bhsa/tf/2021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Computed',\n",
       "  'computed-data',\n",
       "  ('C Computed', 'Call AllComputeds', 'Cs ComputedString')),\n",
       " ('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString')),\n",
       " ('Fabric', 'loading', ('TF',)),\n",
       " ('Locality', 'locality', ('L Locality',)),\n",
       " ('Nodes', 'navigating-nodes', ('N Nodes',)),\n",
       " ('Features',\n",
       "  'node-features',\n",
       "  ('F Feature', 'Fall AllFeatures', 'Fs FeatureString')),\n",
       " ('Search', 'search', ('S Search',)),\n",
       " ('Text', 'text', ('T Text',))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf.fabric import Fabric\n",
    "\n",
    "path = \"bhsa/tf/2021\"\n",
    "features = 'sp prs gn nu vt vs prs_ps prs_gn prs_nu gloss freq_lex freq_occ g_lex_utf8 nametype ls qere_utf8 st language ' + MORPHEMES\n",
    "\n",
    "TF = Fabric(locations=path)\n",
    "api = TF.load(features=features)\n",
    "api.makeAvailableIn(locals())\n",
    "F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.app import use\n",
    "\n",
    "# A = use('bhsa', hoist=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 רֵאשִׁ֖ית {'g_nme_utf8': ' ֜ '}\n",
      "4 אֱלֹהִ֑ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "7 שָּׁמַ֖יִם {'g_nme_utf8': ' ִ֜מ '}\n",
      "11 אָֽרֶץ {'g_nme_utf8': ' ֜ '}\n",
      "14 אָ֗רֶץ {'g_nme_utf8': ' ֜ '}\n",
      "15 הָיְתָ֥ה {'g_vbe_utf8': ' ְתָה '}\n",
      "16 תֹ֨הוּ֙ {'g_nme_utf8': ' ֜ '}\n",
      "18 בֹ֔הוּ {'g_nme_utf8': ' ֜ '}\n",
      "20 חֹ֖שֶׁךְ {'g_nme_utf8': ' ֜ '}\n",
      "22 פְּנֵ֣י {'g_nme_utf8': ' ֵ֜י '}\n",
      "23 תְהֹ֑ום {'g_nme_utf8': ' ֜ '}\n",
      "25 ר֣וּחַ {'g_nme_utf8': ' ֜ '}\n",
      "26 אֱלֹהִ֔ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "27 מְרַחֶ֖פֶת {'g_nme_utf8': ' ֶ֜ת ', 'g_pfm_utf8': ' מְ '}\n",
      "29 פְּנֵ֥י {'g_nme_utf8': ' ֵ֜י '}\n",
      "31 מָּֽיִם {'g_nme_utf8': ' ִ֜מ '}\n",
      "33 יֹּ֥אמֶר {'g_pfm_utf8': ' יֹּ '}\n",
      "34 אֱלֹהִ֖ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "35 יְהִ֣י {'g_pfm_utf8': ' יְ '}\n",
      "36 אֹ֑ור {'g_nme_utf8': ' ֜ '}\n",
      "38 יְהִי {'g_pfm_utf8': ' יְ '}\n",
      "39 אֹֽור {'g_nme_utf8': ' ֜ '}\n",
      "41 יַּ֧רְא {'g_pfm_utf8': ' יַּ '}\n",
      "42 אֱלֹהִ֛ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "45 אֹ֖ור {'g_nme_utf8': ' ֜ '}\n",
      "49 יַּבְדֵּ֣ל {'g_pfm_utf8': ' יַּ '}\n",
      "50 אֱלֹהִ֔ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "51 בֵּ֥ין {'g_nme_utf8': ' ֜ '}\n",
      "53 אֹ֖ור {'g_nme_utf8': ' ֜ '}\n",
      "55 בֵ֥ין {'g_nme_utf8': ' ֜ '}\n",
      "57 חֹֽשֶׁךְ {'g_nme_utf8': ' ֜ '}\n",
      "59 יִּקְרָ֨א {'g_pfm_utf8': ' יִּ '}\n",
      "60 אֱלֹהִ֤ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "63 אֹור֙ {'g_nme_utf8': ' ֜ '}\n",
      "64 יֹ֔ום {'g_nme_utf8': ' ֜ '}\n",
      "68 חֹ֖שֶׁךְ {'g_nme_utf8': ' ֜ '}\n",
      "70 לָ֑יְלָה {'g_nme_utf8': ' ֜ '}\n",
      "72 יְהִי {'g_pfm_utf8': ' יְ '}\n",
      "73 עֶ֥רֶב {'g_nme_utf8': ' ֜ '}\n",
      "75 יְהִי {'g_pfm_utf8': ' יְ '}\n",
      "76 בֹ֖קֶר {'g_nme_utf8': ' ֜ '}\n",
      "77 יֹ֥ום {'g_nme_utf8': ' ֜ '}\n",
      "78 אֶחָֽד {'g_nme_utf8': ' ֜ '}\n",
      "80 יֹּ֣אמֶר {'g_pfm_utf8': ' יֹּ '}\n",
      "81 אֱלֹהִ֔ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "82 יְהִ֥י {'g_pfm_utf8': ' יְ '}\n",
      "83 רָקִ֖יעַ {'g_nme_utf8': ' ֜ '}\n",
      "85 תֹ֣וךְ {'g_nme_utf8': ' ֜ '}\n",
      "87 מָּ֑יִם {'g_nme_utf8': ' ִ֜מ '}\n",
      "89 יהִ֣י {'g_pfm_utf8': ' י '}\n",
      "90 מַבְדִּ֔יל {'g_nme_utf8': ' ֜ ', 'g_pfm_utf8': ' מַ '}\n",
      "91 בֵּ֥ין {'g_nme_utf8': ' ֜ '}\n",
      "92 מַ֖יִם {'g_nme_utf8': ' ִ֜מ '}\n",
      "94 מָֽיִם {'g_nme_utf8': ' ִ֜מ '}\n",
      "96 יַּ֣עַשׂ {'g_pfm_utf8': ' יַּ '}\n",
      "97 אֱלֹהִים֮ {'g_nme_utf8': ' ִ֜ים '}\n",
      "100 רָקִיעַ֒ {'g_nme_utf8': ' ֜ '}\n",
      "102 יַּבְדֵּ֗ל {'g_pfm_utf8': ' יַּ '}\n",
      "103 בֵּ֤ין {'g_nme_utf8': ' ֜ '}\n",
      "105 מַּ֨יִם֙ {'g_nme_utf8': ' ִ֜מ '}\n",
      "108 תַּ֣חַת {'g_nme_utf8': ' ֜ '}\n",
      "111 רָקִ֔יעַ {'g_nme_utf8': ' ֜ '}\n",
      "113 בֵ֣ין {'g_nme_utf8': ' ֜ '}\n",
      "115 מַּ֔יִם {'g_nme_utf8': ' ִ֜מ '}\n",
      "121 רָקִ֑יעַ {'g_nme_utf8': ' ֜ '}\n",
      "123 יְהִי {'g_pfm_utf8': ' יְ '}\n",
      "126 יִּקְרָ֧א {'g_pfm_utf8': ' יִּ '}\n",
      "127 אֱלֹהִ֛ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "130 רָקִ֖יעַ {'g_nme_utf8': ' ֜ '}\n",
      "131 שָׁמָ֑יִם {'g_nme_utf8': ' ִ֜מ '}\n",
      "133 יְהִי {'g_pfm_utf8': ' יְ '}\n",
      "134 עֶ֥רֶב {'g_nme_utf8': ' ֜ '}\n",
      "136 יְהִי {'g_pfm_utf8': ' יְ '}\n",
      "137 בֹ֖קֶר {'g_nme_utf8': ' ֜ '}\n",
      "138 יֹ֥ום {'g_nme_utf8': ' ֜ '}\n",
      "139 שֵׁנִֽי {'g_nme_utf8': ' ֜ '}\n",
      "141 יֹּ֣אמֶר {'g_pfm_utf8': ' יֹּ '}\n",
      "142 אֱלֹהִ֗ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "143 יִקָּו֨וּ {'g_pfm_utf8': ' יִ ', 'g_vbe_utf8': ' וּ '}\n",
      "145 מַּ֜יִם {'g_nme_utf8': ' ִ֜מ '}\n",
      "147 תַּ֤חַת {'g_nme_utf8': ' ֜ '}\n",
      "149 שָּׁמַ֨יִם֙ {'g_nme_utf8': ' ִ֜מ '}\n",
      "151 מָקֹ֣ום {'g_nme_utf8': ' ֜ '}\n",
      "152 אֶחָ֔ד {'g_nme_utf8': ' ֜ '}\n",
      "154 תֵרָאֶ֖ה {'g_pfm_utf8': ' תֵ '}\n",
      "156 יַּבָּשָׁ֑ה {'g_nme_utf8': ' ָ֜ה '}\n",
      "158 יְהִי {'g_pfm_utf8': ' יְ '}\n",
      "161 יִּקְרָ֨א {'g_pfm_utf8': ' יִּ '}\n",
      "162 אֱלֹהִ֤ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "165 יַּבָּשָׁה֙ {'g_nme_utf8': ' ָ֜ה '}\n",
      "166 אֶ֔רֶץ {'g_nme_utf8': ' ֜ '}\n",
      "169 מִקְוֵ֥ה {'g_nme_utf8': ' ֜ '}\n",
      "171 מַּ֖יִם {'g_nme_utf8': ' ִ֜מ '}\n",
      "173 יַמִּ֑ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "175 יַּ֥רְא {'g_pfm_utf8': ' יַּ '}\n",
      "176 אֱלֹהִ֖ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "180 יֹּ֣אמֶר {'g_pfm_utf8': ' יֹּ '}\n",
      "181 אֱלֹהִ֗ים {'g_nme_utf8': ' ִ֜ים '}\n",
      "182 תַּֽדְשֵׁ֤א {'g_pfm_utf8': ' תַּ '}\n",
      "184 אָ֨רֶץ֙ {'g_nme_utf8': ' ֜ '}\n",
      "185 דֶּ֔שֶׁא {'g_nme_utf8': ' ֜ '}\n",
      "186 עֵ֚שֶׂב {'g_nme_utf8': ' ֜ '}\n",
      "187 מַזְרִ֣יעַ {'g_nme_utf8': ' ֜ ', 'g_pfm_utf8': ' מַ '}\n",
      "188 זֶ֔רַע {'g_nme_utf8': ' ֜ '}\n",
      "189 עֵ֣ץ {'g_nme_utf8': ' ֜ '}\n",
      "190 פְּרִ֞י {'g_nme_utf8': ' ֜ '}\n",
      "191 עֹ֤שֶׂה {'g_nme_utf8': ' ֜ '}\n",
      "192 פְּרִי֙ {'g_nme_utf8': ' ֜ '}\n",
      "194 מִינֹ֔ו {'g_nme_utf8': ' ֜ ', 'g_prs_utf8': ' ֹו '}\n",
      "196 זַרְעֹו {'g_nme_utf8': ' ֜ ', 'g_prs_utf8': ' ֹו '}\n",
      "197 בֹ֖ו {'g_prs_utf8': ' ֹו '}\n",
      "200 אָ֑רֶץ {'g_nme_utf8': ' ֜ '}\n"
     ]
    }
   ],
   "source": [
    "for word in F.otype.s(\"word\")[:200]:\n",
    "    data = {}\n",
    "    for f in MORPHEMES.split():\n",
    "        if getattr(F, f).v(word) not in [None, '']:\n",
    "            data[f] = f\" {getattr(F, f).v(word)} \"\n",
    "    if len(data) > 0:\n",
    "        print(word, F.g_word_utf8.v(word), data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used by get_passages()\n",
    "\n",
    "Check whether we have reached the end of a valid passage as defined by \n",
    "passage_size and paragraph markers. If we have, or if a new book (or new \n",
    "chapter in certain books like Psalms), then mark the passage as valid by\n",
    "setting its value to True. \n",
    "In certain cases we will not want to add the current verse to the current\n",
    "passage, so we will set add_verse to False. \n",
    "\"\"\"\n",
    "def valid_passage(passage, verse, passage_size_min, passage_size_max):\n",
    "    is_valid = False\n",
    "    add_verse = True\n",
    "    # Get the string value at the end of the verse. \n",
    "    verse_ending = T.text(verse).split()[-1]\n",
    "    verse_book = L.u(verse, otype='book')[0]\n",
    "    verse_chapter = L.u(verse, otype='chapter')[0]\n",
    "    verse_word_count = len(passage.get_vs_words(verse))\n",
    "    ps_119 = 427315 # node for Psalm 119.\n",
    "    # Check if we've reached a new book, if yes, end the paragraph.\n",
    "    if L.u(passage.verses[-1], otype='book')[0] != verse_book:\n",
    "        is_valid = True \n",
    "        add_verse = False\n",
    "    # Check if the current verse is in the following books.\n",
    "    # Since they lack enough paragraph markers to make meaningful passages,\n",
    "    # we create passages at the chapter level. \n",
    "    elif verse_book in [T.bookNode('Ruth'), T.bookNode('Jonah'), T.bookNode('Ecclesiastes'), T.bookNode('Psalms')]:\n",
    "        if verse_chapter != L.u(passage.verses[-1], otype='chapter')[0]:\n",
    "            is_valid = True \n",
    "            add_verse = False\n",
    "        # If Psalm 119, split up into 8 verse sections to preserve acrostic.\n",
    "        elif verse_chapter == ps_119:\n",
    "            if (verse-1) % 8 == 0:\n",
    "                is_valid = True \n",
    "                add_verse = False\n",
    "    # Otherwise check if we have reached the end of a paragraph. \n",
    "    elif verse_ending in passage.paragraph_markers.keys() \\\n",
    "    and len(passage.get_all_words()) + verse_word_count >= passage_size_min:\n",
    "        is_valid = True\n",
    "\n",
    "    # TODO Optimize this to create meaningful passages.\n",
    "    # Or if the passage is too long.\n",
    "    # ** the len(getAllWords) greatly increases the run time -- we need a way to optimize. \n",
    "    # elif len(passage.get_all_words()) + verse_word_count > passage_size_max:\n",
    "    #     is_valid = True \n",
    "    #     add_verse = False\n",
    "\n",
    "    return is_valid, add_verse\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Used by get_passages\n",
    "\n",
    "Update all of the data of a passage instance once its end verse \n",
    "has been reached. \n",
    "\"\"\"\n",
    "def update_passage_data(passage, rank_scale):\n",
    "    # TODO to print nouns in red. \n",
    "    passage.word_ranks_data = {k:{'occ':0, 'words':set()} for k in rank_scale.keys()}\n",
    "    passage.words = passage.get_all_words()\n",
    "\n",
    "    passage.start_verse = passage.verses[0]\n",
    "    passage.end_verse = passage.verses[-1]\n",
    "    passage.start_word = passage.words[0]\n",
    "    passage.end_word = passage.words[-1]\n",
    "    passage.word_count = len(passage.words)\n",
    "\n",
    "    passage.weight0 = passage.get_passage_weight0()\n",
    "    passage.weight1 = passage.get_passage_weight1(rank_scale)\n",
    "    passage.weight2a = passage.get_passage_weight2(rank_scale)\n",
    "    passage.weight2b = passage.get_passage_weight2(rank_scale, div_all=False)\n",
    "    passage.weight3a = passage.get_passage_weight3(rank_scale)\n",
    "    passage.weight3b = passage.get_passage_weight3(rank_scale, div_all=False)\n",
    "    passage.weight3c = passage.get_passage_weight3(rank_scale, morph=True)\n",
    "\n",
    "    # Update the passage's word frequency and verb data.\n",
    "    for word in passage.words:\n",
    "        # Update the types and stems of verbs present. \n",
    "        if F.sp.v(word) == 'verb':\n",
    "            # if F.vt.v(word) not in c.easy_vtypes:\n",
    "            passage.verb_types_present.add(F.vt.v(word))\n",
    "            # if F.vt.v(word) not in c.easy_vstems:\n",
    "            passage.verb_stems_present.add(F.vs.v(word))\n",
    "        # Update the word_ranks_data dictionary with\n",
    "        # the words in each category.\n",
    "        for rank in rank_scale.keys():\n",
    "            lex_freq = F.freq_lex.v(word)\n",
    "            _range = rank_scale[rank]['range']\n",
    "            if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                passage.word_ranks_data[rank]['occ'] += 1\n",
    "                passage.word_ranks_data[rank]['words'].add(F.voc_lex_utf8.v(word))\n",
    "                    \n",
    "    # Update the passage's start and end reference.\n",
    "    start_ref = T.sectionFromNode(passage.verses[0])\n",
    "    end_ref = T.sectionFromNode(passage.verses[-1])\n",
    "    passage.start_ref = f\"{start_ref[0][:6]} {start_ref[1]}:{start_ref[2]}\"\n",
    "    passage.end_ref = f\"{end_ref[0][:6]} {end_ref[1]}:{end_ref[2]}\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Iterates over verses in the OT and combines them into passages. \n",
    "The function returns a list of Passage objects. \n",
    "\n",
    "rank_scale - a dictionary generated by Ranks().rank_scales()\n",
    "    For example:\n",
    "        rank_scales = Ranks().rank_scales(Ranks().all_ranks)[index]\n",
    "\n",
    "start_node - the verse node at which get_passages will begin. \n",
    "\n",
    "end_node - the verse node at which get_passages will finish executing.\n",
    "\n",
    "passage_size - the minimum words in a passage, unless a chapter is shorter\n",
    "than that (e.g., Psalm 117).\n",
    "\"\"\"\n",
    "def get_passages(\n",
    "    rank_scale, \n",
    "    start_node=0,\n",
    "    end_node=len(F.otype.s('verse')), \n",
    "    passage_size_min=100,\n",
    "    passage_size_max=4000\n",
    "    ):\n",
    "\n",
    "    # A list of all passages.\n",
    "    passages = []\n",
    "\n",
    "    # Initiate the id counter and instantiate the first passage.\n",
    "    passage_id = 1\n",
    "    passage = Passage(id=passage_id)\n",
    "\n",
    "    # Iterate through all verses in the OT. \n",
    "    for verse in F.otype.s('verse')[start_node:end_node]:\n",
    "\n",
    "        # Check if the string is a paragraph marker and if the paragraph is large enough.  \n",
    "        if len(passage.verses) > 1:\n",
    "            valid, add_verse = valid_passage(passage, verse, passage_size_min, passage_size_max)\n",
    "            if valid:\n",
    "\n",
    "                # We have reached the end of the passage so we update all of its attribute values.\n",
    "                if add_verse:\n",
    "                    passage.verses.append(verse)\n",
    "                update_passage_data(passage, rank_scale)\n",
    "                passages.append(passage)\n",
    "                # Begin a new passage. \n",
    "                passage_id += 1\n",
    "                passage = Passage(id=passage_id)\n",
    "\n",
    "                # The current verse is in a new chapter or book so we append it to the\n",
    "                # verses of the newly created passage as its start verse. \n",
    "                if not add_verse:\n",
    "                    passage.verses.append(verse)\n",
    "            # We haven't reached a new passage yet, so add the current verse to its list. \n",
    "            else:\n",
    "                passage.verses.append(verse)\n",
    "\n",
    "        # Add the first verse to the passage. \n",
    "        else:\n",
    "            passage.verses.append(verse)\n",
    "\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "def all_ranks():\n",
    "    start = time.time()\n",
    "    rank_scales = LexRanks().all_ranks[6]\n",
    "    all_passage_rankings = []\n",
    "    for r_s in [rank_scales]:\n",
    "        rank_scale = r_s.get_rank_dict()\n",
    "        all_p = Passages(\n",
    "                passages= get_passages(\n",
    "                rank_scale, \n",
    "                # start_node=0,\n",
    "                # end_node=100, \n",
    "                # passage_size=100\n",
    "            ),\n",
    "            rank_scale=r_s)\n",
    "        all_passage_rankings.append(all_p)\n",
    "        print(r_s.name, \"complete\", time.time()-start)\n",
    "    return all_passage_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_ranks complete 10.357568979263306\n"
     ]
    }
   ],
   "source": [
    "passages = all_ranks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = passages[0].weight_sort3a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in list(passages.items())[:10]:\n",
    "    print(vars(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class that contains a Hebrew passage, consisting of paragraphs \n",
    "as marked by a petach (פ) or samech (ס) in the Masoretic Text. If \n",
    "a book like Psalms, which lacks paragaph markers, is encountered,\n",
    "the passages are split at the chapter level. \n",
    "\"\"\"\n",
    "class Passage:\n",
    "\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.verses = [] # a list of verse node ints. \n",
    "        self.words = [] # a list of word node ints. \n",
    "        self.start_word = 0\n",
    "        self.end_word = 0\n",
    "        self.start_verse = 0\n",
    "        self.end_verse = 0\n",
    "        self.word_count = 0\n",
    "        self.weight0 = 0\n",
    "        self.weight1 = 0\n",
    "        self.weight2a = 0 # all words denom\n",
    "        self.weight2b = 0 # unique words\n",
    "        self.weight3a = 0 # all word denom\n",
    "        self.weight3b = 0\n",
    "        self.weight3c = 0 #3a with morph\n",
    "        self.verb_types_present = set()\n",
    "        self.verb_stems_present = set()\n",
    "        self.word_ranks_data = {}\n",
    "        self.start_ref = ''\n",
    "        self.end_ref = ''\n",
    "\n",
    "    paragraph_markers = {'פ': 'open', 'ס': 'closed'}\n",
    "\n",
    "    # Returns a list of all words present in the passage.\n",
    "    def get_all_words(self):\n",
    "        words = []\n",
    "        for verse in self.verses:\n",
    "            for word in L.i(verse, otype='word'):\n",
    "                words.append(word)\n",
    "        return words\n",
    "\n",
    "    # Returns a list of all words present in a specified verse in the passage.\n",
    "    def get_vs_words(self, verse):\n",
    "        verse_words = [w for w in L.i(verse, otype='word')]\n",
    "        return verse_words\n",
    "\n",
    "    # Returns a String of all the text in the passage.\n",
    "    def get_text(self):\n",
    "        return T.text(self.verses, fmt='text-orig-full')\n",
    "\n",
    "    \"\"\"\n",
    "    get_vs_weights() returns a dictionary mapping each verse node in \n",
    "    the passage to a weight. It takes rank_scale as input, an instance\n",
    "    of Classify(args).rank_scale() (see notes in Classify for instantiaion).\n",
    "    \"\"\"\n",
    "    def get_vs_weights(self, rank_scale):\n",
    "        # A dictionary mapping verse nodes to weights.\n",
    "        verse_weights = {}\n",
    "        # Iterate over verses in the passage.\n",
    "        for verse in self.verses:\n",
    "            verse_weight = 0\n",
    "            words = self.get_vs_words(verse)\n",
    "            # Add the scaled word weights to the verse's total weight.\n",
    "            for word in words:\n",
    "                if F.voc_lex_utf8.v(word) not in Classify().stop_words:\n",
    "                    for rank in rank_scale.keys():\n",
    "                        lex_freq = F.freq_lex.v(word)\n",
    "                        _range = rank_scale[rank]['range']\n",
    "                        if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                            verse_weight += rank_scale[rank]['weight']\n",
    "            # Add the verse's weight to the dictionary at this verse's key. \n",
    "            verse_weight /= len(words)\n",
    "            verse_weights[verse] = round(verse_weight, 4)\n",
    "        \n",
    "        return verse_weights\n",
    "\n",
    "    # Simply add the freq_lex of each word to weight.\n",
    "    def get_passage_weight0(self):\n",
    "        total_weight = 0\n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            if F.voc_lex_utf8.v(word) not in Classify().stop_words:\n",
    "                # Subtract 10000 to penalize rare words. \n",
    "                total_weight += 10000 - F.freq_lex.v(word)\n",
    "        total_weight /= len(self.words)\n",
    "        \n",
    "        return round(total_weight, 4)\n",
    "\n",
    "    def get_passage_weight1(self, rank_scale):\n",
    "        total_weight = 0\n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            if F.voc_lex_utf8.v(word) not in Classify().stop_words:\n",
    "                # Iterate over the ranks present in the rank scale. \n",
    "                for rank in rank_scale.keys():\n",
    "                    lex_freq = F.freq_lex.v(word)\n",
    "                    _range = rank_scale[rank]['range']\n",
    "                    if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                        # Give a half penalty for proper nouns. \n",
    "                        if F.sp.v(word) == 'nmpr': # proper noun\n",
    "                            total_weight += (rank_scale[rank]['weight']) / 2\n",
    "                        # Give a full penalty for other word types. \n",
    "                        else:\n",
    "                            total_weight += rank_scale[rank]['weight']\n",
    "        total_weight /= len(self.words)\n",
    "        \n",
    "        return round(total_weight, 4)\n",
    "\n",
    "    # Only penalize once per lexical value.  \n",
    "    def get_passage_weight2(self, rank_scale, div_all=True):\n",
    "        total_weight = 0\n",
    "        unique_words = set()\n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            lex = F.voc_lex_utf8.v(word)\n",
    "            if lex not in Classify().stop_words and lex not in unique_words:\n",
    "                # Iterate over the ranks present in the rank scale. \n",
    "                for rank in rank_scale.keys():\n",
    "                    lex_freq = F.freq_lex.v(word)\n",
    "                    _range = rank_scale[rank]['range']\n",
    "                    if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                        # Give a half penalty for proper nouns. \n",
    "                        if F.sp.v(word) == 'nmpr': # proper noun\n",
    "                            total_weight += (rank_scale[rank]['weight']) / 2\n",
    "                        # Give a full penalty for other word types. \n",
    "                        else:\n",
    "                            total_weight += rank_scale[rank]['weight']\n",
    "                unique_words.add(lex)\n",
    "        # Compare using all words as denominator vs. unique words.\n",
    "        if div_all:\n",
    "            total_weight /= len(self.words)\n",
    "        else:\n",
    "            total_weight /= len(unique_words)\n",
    "        \n",
    "        return round(total_weight, 4)\n",
    "\n",
    "    # Decrease penalty for each occurance. \n",
    "    def get_passage_weight3(self, rank_scale, div_all=True, morph=False):\n",
    "        word_weights = {}\n",
    "        verb_count = 0\n",
    "        verb_weight = 0\n",
    "        min_penalty = 1.7 # min penalty for rare words and proper nouns. \n",
    "        # Iterate over words in the passage.\n",
    "        for word in self.words:\n",
    "            lex = F.voc_lex_utf8.v(word)\n",
    "            if lex not in Classify().stop_words:\n",
    "                # Add partial penalty for reocurring words. \n",
    "                if lex in word_weights.keys():\n",
    "                    # Only gradually decrease penalty for rarer words. \n",
    "                    # Decreases by 1 point per occurance. \n",
    "                    word_weights[lex]['count'] += 1\n",
    "                    if F.freq_lex.v(word) < 100:\n",
    "                        count = word_weights[lex]['count']\n",
    "                        penalty = word_weights[lex]['penalty']\n",
    "                        new_weight = penalty - count \n",
    "                        added_weight = new_weight if new_weight >= min_penalty else min_penalty\n",
    "                        word_weights[lex]['weight'] += added_weight\n",
    "                    else:\n",
    "                        word_weights[lex]['weight'] += word_weights[lex]['penalty']\n",
    "                # Add full penalty for the first occurance. \n",
    "                else:\n",
    "                    # Add word to hash table\n",
    "                    word_weights[lex] = {'count':0, 'weight':0, 'penalty':0}\n",
    "                    # Iterate over the ranks present in the rank scale. \n",
    "                    for rank in rank_scale.keys():\n",
    "                        lex_freq = F.freq_lex.v(word)\n",
    "                        _range = rank_scale[rank]['range']\n",
    "                        if lex_freq >= _range[0] and lex_freq < _range[1]:\n",
    "                            # Give a half penalty for proper nouns. \n",
    "                            _penalty = rank_scale[rank]['weight']\n",
    "                            if F.sp.v(word) == 'nmpr' and _penalty > min_penalty: # proper noun\n",
    "                                word_weights[lex]['penalty'] = int(math.ceil(_penalty / 2))\n",
    "                            # Give a full penalty for other word types. \n",
    "                            else:\n",
    "                                word_weights[lex]['penalty'] = _penalty\n",
    "                    word_weights[lex]['weight'] += word_weights[lex]['penalty']\n",
    "                    word_weights[lex]['count'] += 1\n",
    "                # If we're penalizing for morphology\n",
    "                if morph:\n",
    "                    if F.sp.v(word) == 'verb':\n",
    "                        verb_count += 1\n",
    "                        verb_weight += MorphRank.stem_map.get(F.vs.v(word),0) + MorphRank.tense_map[F.vt.v(word)]\n",
    "\n",
    "        # Get the sum of all word weights. \n",
    "        total_weight = sum([w for w in [word_weights[k]['weight'] for k in word_weights.keys()]])\n",
    "        # Compare using all words as denominator vs. unique words.\n",
    "        if div_all:\n",
    "            total_weight = total_weight / len(self.words) + (verb_weight / len(self.words))\n",
    "        else:\n",
    "            total_weight /= len(word_weights)\n",
    "        \n",
    "        return round(total_weight, 4)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "A class to store passages. It includes methods to sort the passages\n",
    "by attributes such as word count, weight, and canonical order. It\n",
    "also stores the rank scale used to create the passage list. The passages\n",
    "will be stored in order_sorted by default. \n",
    "\"\"\"\n",
    "class Passages:\n",
    "    \n",
    "    def __init__(self, passages, rank_scale={}):\n",
    "        self.rank_scale = rank_scale\n",
    "        self.order_sorted = passages\n",
    "        self.word_count_sorted = self.word_count_sort()\n",
    "        self.weight_sorted0 = self.weight_sort0()\n",
    "        self.weight_sorted1 = self.weight_sort1()\n",
    "        self.weight_sorted2a = self.weight_sort2a()\n",
    "        self.weight_sorted2b = self.weight_sort2b()\n",
    "        self.weight_sorted3a = self.weight_sort3a()\n",
    "        self.weight_sorted3b = self.weight_sort3b()\n",
    "        self.weight_sorted3c = self.weight_sort3c()\n",
    "    \n",
    "    def word_count_sort(self):\n",
    "        return sorted(self.order_sorted, key=lambda p: p.word_count)\n",
    "\n",
    "    \"\"\" For weight sorts I use a dict mapping Passage objects to their rank\n",
    "    number so that I can then compare all the weight sorts in a dataframe. \"\"\"\n",
    "    def weight_sort0(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight0)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort1(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight1)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort2a(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight2a)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort2b(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight2b)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort3a(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight3a)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    def weight_sort3b(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight3b)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    # Morph\n",
    "    def weight_sort3c(self):\n",
    "        sorted_list = sorted(self.order_sorted, key=lambda p: p.weight3c)\n",
    "        return {sorted_list[i]:i for i in range(len(sorted_list))}\n",
    "\n",
    "    # A function to display the rank scale as a multi-line string. \n",
    "    def print_scale(self):\n",
    "        scale = self.rank_scale\n",
    "        output_text = \"\"\n",
    "        for i, rank in enumerate(scale.ranks):\n",
    "            _range = scale.ranges[i]\n",
    "            weight = scale.weights[i]\n",
    "            output = f\"{weight}\\t{_range[0]}-{_range[1]} occ\"\n",
    "            output_text += f\"{rank}:   \\t{output}\\n\"\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A class with data used to assign difficulty weights to passages based\n",
    "on the lexical frequencies of words in the passage. \n",
    "\n",
    "ranks: a list of string categories for lexical frequency ranges\n",
    "ranges: a 2D list of the numeric range for each rank\n",
    "weights: a list of the weight penalties assigned per word for each rank\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Rank:\n",
    "\n",
    "    def __init__(self, name, ranks, ranges, weights):\n",
    "        self.name: str = name\n",
    "        self.ranks: list[str] = ranks \n",
    "        self.ranges: list[list[int]] = ranges \n",
    "        self.weights: list[float] = weights \n",
    "\n",
    "    # Auxiliary function to create a single rank_scale dictionary.\n",
    "    def get_rank_dict(self) -> dict:\n",
    "        rank_dict = {}\n",
    "        for i in range(len(self.ranks)):\n",
    "            rank_dict[self.ranks[i]] = {\n",
    "                'range': self.ranges[i],\n",
    "                'weight': self.weights[i]\n",
    "            }\n",
    "        return rank_dict \n",
    "\n",
    "\n",
    "\"\"\" \n",
    "A class to store different ranking scales. \n",
    "\"\"\"\n",
    "class LexRanks:\n",
    "\n",
    "    # Using 2-elem lists is far faster than searching entire ranges. \n",
    "    # Rather than if i in range(), check if i > l[0] and <= l[1].\n",
    "    # Using this method scales runtime from ~0:04:30 to ~0:00:15.\n",
    "    _3_ranks = Rank(\n",
    "        \"3_ranks\",\n",
    "        ['Frequent', 'Uncommon', 'Rare'],\n",
    "        [\n",
    "            [100, 51000],\n",
    "            [10, 100],\n",
    "            [1, 10],\n",
    "        ],\n",
    "        [1, 3, 7]\n",
    "    )\n",
    "   \n",
    "    _4_ranks = Rank(\n",
    "        \"4_ranks\",\n",
    "        ['Frequent', 'Medium', 'Uncommon', 'Rare'],\n",
    "        [\n",
    "            [100, 51000],\n",
    "            [50, 100],\n",
    "            [10, 50],\n",
    "            [1, 10],\n",
    "        ],\n",
    "        [1, 4, 5, 8]\n",
    "    )\n",
    "\n",
    "    _5_ranks_a = Rank(\n",
    "        \"5_ranks_a\",\n",
    "        ['Frequent', 'Common', 'Medium', 'Uncommon', 'Rare'],\n",
    "        [\n",
    "            [500, 51000],\n",
    "            [250, 500],\n",
    "            [150, 250],\n",
    "            [50, 150],\n",
    "            [1, 50],\n",
    "        ],\n",
    "        [1, 2, 3, 5, 8]\n",
    "    )\n",
    "\n",
    "    _5_ranks_b = Rank(\n",
    "        \"5_ranks_b\",\n",
    "        ['Frequent', 'Common', 'Infrequent', 'Rare', 'Scarce'],\n",
    "        [\n",
    "            [200, 51000],\n",
    "            [100, 200],\n",
    "            [50, 100],\n",
    "            [20, 50],\n",
    "            [1, 20],\n",
    "        ],\n",
    "        [1, 1.5, 3, 5, 8]\n",
    "    )\n",
    "\n",
    "    _7_ranks = Rank(\n",
    "        \"7_ranks\",\n",
    "        ['Abundant', 'Frequent', 'Common', 'Average', 'Uncommon', 'Rare', 'Scarce'],\n",
    "        [\n",
    "            [800, 51000],\n",
    "            [400, 800],\n",
    "            [200, 400],\n",
    "            [100, 200],\n",
    "            [50, 100],\n",
    "            [15, 50],\n",
    "            [1, 15],\n",
    "        ],\n",
    "        [1, 1.1, 1.3, 1.7, 3, 5.5, 8.5]\n",
    "    )\n",
    "\n",
    "    _9_ranks = Rank(\n",
    "        \"9_ranks\",\n",
    "        ['Abundant', 'Frequent', 'Common', 'Average', 'Uncommon', 'Rare', 'Scarce', 'Scarcer', 'Scarcest'],\n",
    "        [\n",
    "            [1000, 51000],\n",
    "            [400, 1000],\n",
    "            [200, 400],\n",
    "            [100, 200],\n",
    "            [50, 100],\n",
    "            [30, 50],\n",
    "            [20, 30],\n",
    "            [10, 20],\n",
    "            [1, 10]\n",
    "        ],\n",
    "        [1, 1.1, 1.3, 1.7, 3, 5.5, 8, 9, 10]\n",
    "    )\n",
    "\n",
    "    _10_ranks = Rank(\n",
    "        \"10_ranks\",\n",
    "        ['Abundant', 'Frequent', 'Common', 'Average', 'Uncommon', 'Rare', 'Rarer', 'Scarce', 'Scarcer', 'Scarcest'],\n",
    "        [\n",
    "            [1000, 51000],\n",
    "            [400, 1000],\n",
    "            [200, 400],\n",
    "            [100, 200],\n",
    "            [50, 100],\n",
    "            [40, 50],\n",
    "            [30, 40],\n",
    "            [20, 30],\n",
    "            [10, 20],\n",
    "            [1, 10]\n",
    "        ],\n",
    "        [1, 1.1, 1.3, 1.7, 3, 5.5, 7, 8, 9, 10]\n",
    "    )\n",
    "\n",
    "    all_ranks: list[Rank] = [\n",
    "        _3_ranks,\n",
    "        _4_ranks,\n",
    "        _5_ranks_a,\n",
    "        _5_ranks_b,\n",
    "        _7_ranks,\n",
    "        _9_ranks,\n",
    "        _10_ranks\n",
    "    ]\n",
    "\n",
    "\n",
    "# Include morphology penalties.\n",
    "class MorphRank:\n",
    "\n",
    "    other: int = 8\n",
    "    base: int = 0\n",
    "\n",
    "    stem_map: dict[str,int] = {\n",
    "        'hif':2,\t#hif‘il\n",
    "        'hit':3,\t#hitpa“el\n",
    "        'htpo':other,\t#hitpo“el\n",
    "        'hof':5,\t#hof‘al\n",
    "        'nif':3,\t#nif‘al\n",
    "        'piel':2,\t#pi“el\n",
    "        'poal':other,\t#po“al\n",
    "        'poel':other,\t#po“el\n",
    "        'pual':5,\t#pu“al\n",
    "        'qal':base\t#qal\n",
    "    }\n",
    "    tense_map: dict[str,int] = {\n",
    "        'perf':base,\t#perfect\n",
    "        'impf':2,\t#imperfect\n",
    "        'wayq':base,\t#wayyiqtol\n",
    "        'impv':3.5,\t#imperative\n",
    "        'infa':5,\t#infinitive (absolute)\n",
    "        'infc':2,\t#infinitive (construct)\n",
    "        'ptca':3,\t#participle\n",
    "        'ptcp':5,\t#participle (passive)\n",
    "    }   \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A class that contains data to help assign difficulty weights to \n",
    "any portion of Hebrew text that has more than one word. \n",
    "\"\"\"\n",
    "class Classify:\n",
    "    \"\"\" \n",
    "    Notes on stop_words_types and other exclusion lists. \n",
    "\n",
    "    Most prepositions, articles, and conjunctions don't\n",
    "    add any meaningul weight to a text and could thus be exlcuded.\n",
    "    \n",
    "    Example use:\n",
    "    words = [w for w in passage if F.sp.v(w) not in stop_words_types]\n",
    "    \n",
    "    Note: the only Heb article is 'הַ' with 30,386 occurences. There are some \n",
    "    preps and conjs that have few occurences, so I recommend not using\n",
    "    stop_words_types when weighing passages and using stop_words instead.\n",
    "    \"\"\"\n",
    "    stop_words_types: list[str] = ['prep', 'art', 'conj']\n",
    "    # Check if F.voc_lex_utf8.v(word) is in this list. If\n",
    "    # so it can be excluded since it occurs so often. \n",
    "    stop_words: list[str] = ['אֵת', 'בְּ', 'לְ', 'הַ', 'וְ']\n",
    "    # If you take verb data into account when weighing a\n",
    "    # paragraph, these common types could be excluded. \n",
    "    easy_vtypes: list[str] = ['perf', 'impf', 'wayq']\n",
    "    easy_vstems: list[str] = ['qal', 'hif', 'nif', 'piel']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20e40d8fc09a6690434ad602c7eb2d8de15d36ec466bfbfb0de97c7c540d7363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
